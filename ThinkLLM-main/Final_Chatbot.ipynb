{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import webrtcvad\n",
    "import numpy as np\n",
    "class AudioRecorder:\n",
    "    def __init__(self):\n",
    "        # Audio configuration\n",
    "        self.FORMAT = pyaudio.paInt16\n",
    "        self.CHANNELS = 1\n",
    "        self.RATE = 16000  # WebRTC VAD requires 16000Hz\n",
    "        self.CHUNK = 320   # 30ms at 16000Hz - WebRTC VAD expects 10, 20, or 30ms frames\n",
    "        self.SILENCE_THRESHOLD = 2  # Number of silent chunks before stopping\n",
    "        \n",
    "        # Initialize PyAudio\n",
    "        self.audio = pyaudio.PyAudio()\n",
    "        \n",
    "        # Initialize VAD\n",
    "        self.vad = webrtcvad.Vad()\n",
    "        self.vad.set_mode(1)  # 0: Least aggressive, 3: Most aggressive\n",
    "        \n",
    "    def is_speech(self, frame):\n",
    "        \"\"\"Check if a frame contains speech.\"\"\"\n",
    "        try:\n",
    "            return self.vad.is_speech(frame, self.RATE)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame: {e}\")\n",
    "            return False\n",
    "            \n",
    "    def record_audio(self, silence_timeout=2):\n",
    "        \"\"\"Record audio when speech is detected.\"\"\"\n",
    "        frames = []\n",
    "        recording = False\n",
    "        silent_chunks = 0\n",
    "        \n",
    "        # Open stream\n",
    "        stream = self.audio.open(\n",
    "            format=self.FORMAT,\n",
    "            channels=self.CHANNELS,\n",
    "            rate=self.RATE,\n",
    "            input=True,\n",
    "            frames_per_buffer=self.CHUNK\n",
    "        )\n",
    "        \n",
    "        print(\"Listening for speech...\")\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                frame = stream.read(self.CHUNK, exception_on_overflow=False)\n",
    "                \n",
    "                # Check if frame contains speech\n",
    "                is_speech = self.is_speech(frame)\n",
    "                \n",
    "                if is_speech:\n",
    "                    if not recording:\n",
    "                        print(\"Speech detected - Recording started.\")\n",
    "                        recording = True\n",
    "                    frames.append(frame)\n",
    "                    silent_chunks = 0\n",
    "                elif recording:\n",
    "                    silent_chunks += 1\n",
    "                    frames.append(frame)\n",
    "                    \n",
    "                    # Stop recording after silence_timeout seconds of silence\n",
    "                    if silent_chunks > (silence_timeout * self.RATE) // self.CHUNK:\n",
    "                        print(\"Silence detected - Recording stopped.\")\n",
    "                        break\n",
    "                        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nRecording interrupted by user\")\n",
    "        finally:\n",
    "            # Clean up\n",
    "            stream.stop_stream()\n",
    "            stream.close()\n",
    "            \n",
    "        return frames\n",
    "        \n",
    "    def get_audio_data(self, frames):\n",
    "        \"\"\"\n",
    "        Convert recorded frames to numpy array of audio data.\n",
    "        \"\"\"\n",
    "        if not frames:\n",
    "            print(\"No audio frames to process\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Combine all frames into a single bytes object\n",
    "            audio_data = b''.join(frames)\n",
    "            \n",
    "            # Convert bytes to numpy array\n",
    "            audio_samples = np.frombuffer(audio_data, dtype=np.int16)\n",
    "            return audio_samples\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing audio data: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up PyAudio resources.\"\"\"\n",
    "        self.audio.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder = AudioRecorder()\n",
    "try:\n",
    "    frames = recorder.record_audio()\n",
    "    audio_data = recorder.get_audio_data(frames)\n",
    "    if audio_data is not None and audio_data.size > 0:\n",
    "        # Use audio_data with your speech-to-text system\n",
    "        print(audio_data)\n",
    "        # For example with speech_recognition:\n",
    "        # recognizer.recognize_google(audio_data)\n",
    "\n",
    "        from transformers import pipeline\n",
    "        asr = pipeline(\"automatic-speech-recognition\", \n",
    "                                model=\"openai/whisper-medium\",\n",
    "                                device='cpu')\n",
    "        transcription = asr(audio_data)\n",
    "        print(\"Transcription:\", transcription['text'])\n",
    "\n",
    "        pass\n",
    "finally:\n",
    "    recorder.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import webrtcvad\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import io\n",
    "\n",
    "class AudioRecorder:\n",
    "    def __init__(self):\n",
    "        # Audio configuration\n",
    "        self.FORMAT = pyaudio.paFloat32  # Changed to float32 for better compatibility\n",
    "        self.CHANNELS = 1\n",
    "        self.RATE = 16000  # WebRTC VAD requires 16000Hz\n",
    "        self.CHUNK = 320   # 30ms at 16000Hz\n",
    "        self.SILENCE_THRESHOLD = 2\n",
    "        \n",
    "        # Initialize PyAudio\n",
    "        self.audio = pyaudio.PyAudio()\n",
    "        \n",
    "        # Initialize VAD\n",
    "        self.vad = webrtcvad.Vad()\n",
    "        self.vad.set_mode(1)\n",
    "        \n",
    "        # Initialize Whisper\n",
    "        self.asr = pipeline(\"automatic-speech-recognition\", \n",
    "                          model=\"openai/whisper-medium\",\n",
    "                          device='cuda')\n",
    "    \n",
    "    def is_speech(self, frame):\n",
    "        \"\"\"Check if a frame contains speech.\"\"\"\n",
    "        try:\n",
    "            # Convert float32 to int16 for VAD\n",
    "            frame_int16 = (np.frombuffer(frame, dtype=np.float32) * 32767).astype(np.int16).tobytes()\n",
    "            return self.vad.is_speech(frame_int16, self.RATE)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame: {e}\")\n",
    "            return False\n",
    "            \n",
    "    def record_audio(self, silence_timeout=2):\n",
    "        \"\"\"Record audio when speech is detected.\"\"\"\n",
    "        frames = []\n",
    "        recording = False\n",
    "        silent_chunks = 0\n",
    "        \n",
    "        stream = self.audio.open(\n",
    "            format=self.FORMAT,\n",
    "            channels=self.CHANNELS,\n",
    "            rate=self.RATE,\n",
    "            input=True,\n",
    "            frames_per_buffer=self.CHUNK\n",
    "        )\n",
    "        \n",
    "        print(\"Listening for speech...\")\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                frame = stream.read(self.CHUNK, exception_on_overflow=False)\n",
    "                \n",
    "                if self.is_speech(frame):\n",
    "                    if not recording:\n",
    "                        print(\"Speech detected - Recording started.\")\n",
    "                        recording = True\n",
    "                    frames.append(frame)\n",
    "                    silent_chunks = 0\n",
    "                elif recording:\n",
    "                    silent_chunks += 1\n",
    "                    frames.append(frame)\n",
    "                    \n",
    "                    if silent_chunks > (silence_timeout * self.RATE) // self.CHUNK:\n",
    "                        print(\"Silence detected - Recording stopped.\")\n",
    "                        break\n",
    "                        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nRecording interrupted by user\")\n",
    "        finally:\n",
    "            stream.stop_stream()\n",
    "            stream.close()\n",
    "            \n",
    "        return frames\n",
    "        \n",
    "    def prepare_audio_for_whisper(self, frames):\n",
    "        \"\"\"Convert and prepare audio data for Whisper model.\"\"\"\n",
    "        if not frames:\n",
    "            print(\"No audio frames to process\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Combine frames and convert to numpy array\n",
    "            audio_data = np.frombuffer(b''.join(frames), dtype=np.float32)\n",
    "            \n",
    "            # Normalize audio\n",
    "            audio_data = audio_data / np.max(np.abs(audio_data))\n",
    "            \n",
    "            # Ensure sample rate is correct (Whisper expects 16kHz)\n",
    "            if self.RATE != 16000:\n",
    "                audio_data = librosa.resample(audio_data, orig_sr=self.RATE, target_sr=16000)\n",
    "            \n",
    "            return audio_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing audio data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def transcribe_audio(self, audio_data):\n",
    "        \"\"\"Transcribe audio using Whisper model.\"\"\"\n",
    "        try:\n",
    "            if audio_data is not None and len(audio_data) > 0:\n",
    "                # Process with Whisper\n",
    "                transcription = self.asr({\"sampling_rate\": self.RATE, \"raw\": audio_data})\n",
    "                return transcription['text']\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error during transcription: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up PyAudio resources.\"\"\"\n",
    "        self.audio.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain_ollama import ChatOllama\n",
    "from queue import Queue\n",
    "import re\n",
    "\n",
    "class AsyncSentenceQueue:\n",
    "    def __init__(self):\n",
    "        self.queue = asyncio.Queue()\n",
    "        self.current_sentence = \"\"\n",
    "        \n",
    "    async def put(self, text: str):\n",
    "        \"\"\"Add text and split into sentences when possible\"\"\"\n",
    "        self.current_sentence += text\n",
    "        sentences = re.split(r'([.!?]+)', self.current_sentence)\n",
    "        \n",
    "        # Process complete sentences\n",
    "        while len(sentences) >= 2:  # Need both sentence content and separator\n",
    "            sentence = sentences.pop(0) + sentences.pop(0)  # Combine with separator\n",
    "            if sentence.strip():  # Only queue non-empty sentences\n",
    "                await self.queue.put(sentence)\n",
    "        \n",
    "        # Store remaining incomplete sentence\n",
    "        self.current_sentence = ''.join(sentences)\n",
    "    \n",
    "    async def get(self):\n",
    "        \"\"\"Get next complete sentence from queue\"\"\"\n",
    "        return await self.queue.get()\n",
    "    \n",
    "    def task_done(self):\n",
    "        \"\"\"Mark a queue item as done\"\"\"\n",
    "        self.queue.task_done()\n",
    "    \n",
    "    async def finish(self):\n",
    "        \"\"\"Put any remaining text into queue\"\"\"\n",
    "        if self.current_sentence.strip():\n",
    "            await self.queue.put(self.current_sentence)\n",
    "            self.current_sentence = \"\"\n",
    "\n",
    "async def generate_text(text: str, sentence_queue: AsyncSentenceQueue):\n",
    "    \"\"\"Generate text and put sentences into queue\"\"\"\n",
    "    model = ChatOllama(model=\"llama3.2:1b\")\n",
    "    try:\n",
    "        stream = model.stream(text)\n",
    "        for chunk in stream:\n",
    "            await sentence_queue.put(chunk.content)\n",
    "            await asyncio.sleep(0)  # Yield control\n",
    "        await sentence_queue.finish()  # Queue any remaining text\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"\\nOutput generation cancelled.\")\n",
    "        raise\n",
    "\n",
    "async def display_queue(sentence_queue: AsyncSentenceQueue):\n",
    "    \"\"\"Display sentences from queue with delay for effect\"\"\"\n",
    "    try:\n",
    "        while True:\n",
    "            sentence = await sentence_queue.get()\n",
    "            print(sentence, end='', flush=True)\n",
    "            await asyncio.sleep(0.5)  # Artificial delay for queuing effect\n",
    "            sentence_queue.task_done()\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"\\nDisplay task cancelled.\")\n",
    "        raise\n",
    "\n",
    "async def main():\n",
    "    sentence_queue = AsyncSentenceQueue()\n",
    "    \n",
    "    # Create tasks\n",
    "    generator_task = asyncio.create_task(generate_text(\"tell me a story\", sentence_queue))\n",
    "    display_task = asyncio.create_task(display_queue(sentence_queue))\n",
    "    \n",
    "    # Wait for 5 seconds before cancelling\n",
    "    try:\n",
    "        await asyncio.sleep(5)\n",
    "        print(\"\\nCancelling tasks...\")\n",
    "        generator_task.cancel()\n",
    "        display_task.cancel()\n",
    "        await asyncio.gather(generator_task, display_task, return_exceptions=True)\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"Main task cancelled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    recorder = AudioRecorder()\n",
    "    try:\n",
    "        # Record audio\n",
    "        frames = recorder.record_audio()\n",
    "        \n",
    "        # Prepare audio for Whisper\n",
    "        audio_data = recorder.prepare_audio_for_whisper(frames)\n",
    "        \n",
    "        # Transcribe\n",
    "        if audio_data is not None:\n",
    "            transcription = recorder.transcribe_audio(audio_data)\n",
    "            if transcription:\n",
    "                print(\"Transcription:\", transcription)\n",
    "            else:\n",
    "                print(\"No transcription available\")\n",
    "    finally:\n",
    "        recorder.cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
