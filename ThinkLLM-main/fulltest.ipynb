{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "from transformers import pipeline\n",
    "import time\n",
    "from collections import deque\n",
    "from threading import Thread, Event\n",
    "import queue\n",
    "import asyncio\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    def __init__(self, \n",
    "                 sample_rate=16000,\n",
    "                 chunk_duration=0.1,  # Smaller chunks for faster response\n",
    "                 pause_threshold=0.5,\n",
    "                 energy_threshold=0.02,\n",
    "                 max_duration=3):\n",
    "        \n",
    "        self.sample_rate = sample_rate\n",
    "        self.chunk_duration = chunk_duration\n",
    "        self.chunk_size = int(sample_rate * chunk_duration)\n",
    "        self.pause_threshold = pause_threshold\n",
    "        self.energy_threshold = energy_threshold\n",
    "        self.max_duration = max_duration\n",
    "        \n",
    "        # Use queue for thread-safe audio processing\n",
    "        self.audio_queue = queue.Queue()\n",
    "        self.stop_recording = Event()\n",
    "        \n",
    "        # Initialize ASR model only once\n",
    "        self.asr = pipeline(\"automatic-speech-recognition\", \n",
    "                          model=\"openai/whisper-small\",\n",
    "                          device='cpu')\n",
    "        \n",
    "        # Use deque with maxlen for automatic memory management\n",
    "        max_chunks = int(max_duration / chunk_duration)\n",
    "        self.audio_buffer = deque(maxlen=max_chunks)\n",
    "\n",
    "    def _calculate_energy(self, audio_chunk):\n",
    "        # Vectorized energy calculation\n",
    "        return np.mean(np.abs(audio_chunk))\n",
    "\n",
    "    def _record_audio(self):\n",
    "        \"\"\"Record audio in a separate thread\"\"\"\n",
    "        with sd.InputStream(samplerate=self.sample_rate,\n",
    "                          channels=1,\n",
    "                          dtype=np.float32,\n",
    "                          blocksize=self.chunk_size,\n",
    "                          callback=self._audio_callback):\n",
    "            self.stop_recording.wait()\n",
    "\n",
    "    def _audio_callback(self, indata, frames, time_info, status):\n",
    "        \"\"\"Callback for audio stream processing\"\"\"\n",
    "        if status:\n",
    "            print(f'Error: {status}')\n",
    "        self.audio_queue.put(indata.copy())\n",
    "\n",
    "    def _process_audio_chunk(self, audio_chunk):\n",
    "        \"\"\"Process a single chunk of audio data\"\"\"\n",
    "        energy = self._calculate_energy(audio_chunk)\n",
    "        if energy > self.energy_threshold:\n",
    "            self.audio_buffer.append(audio_chunk.flatten())\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def record_and_transcribe(self):\n",
    "        \"\"\"Main method to record and transcribe audio\"\"\"\n",
    "        print(\"Listening...\")\n",
    "        \n",
    "        # Start recording thread\n",
    "        recording_thread = Thread(target=self._record_audio)\n",
    "        recording_thread.start()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        silence_start = None\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                # Check max duration\n",
    "                if time.time() - start_time > self.max_duration:\n",
    "                    break\n",
    "\n",
    "                # Get audio chunk from queue with timeout\n",
    "                try:\n",
    "                    audio_chunk = self.audio_queue.get(timeout=0.1)\n",
    "                except queue.Empty:\n",
    "                    continue\n",
    "\n",
    "                # Process the chunk\n",
    "                has_speech = self._process_audio_chunk(audio_chunk)\n",
    "                \n",
    "                # Pause detection logic\n",
    "                if not has_speech:\n",
    "                    if silence_start is None:\n",
    "                        silence_start = time.time()\n",
    "                    elif time.time() - silence_start >= self.pause_threshold:\n",
    "                        break\n",
    "                else:\n",
    "                    silence_start = None\n",
    "\n",
    "        finally:\n",
    "            # Clean up\n",
    "            self.stop_recording.set()\n",
    "            recording_thread.join()\n",
    "\n",
    "        # Process recorded audio\n",
    "        if len(self.audio_buffer) > 0:\n",
    "            # Efficient concatenation of all audio chunks\n",
    "            audio_data = np.concatenate(self.audio_buffer)\n",
    "            \n",
    "            # Transcribe\n",
    "            transcription = self.asr(audio_data)\n",
    "            print(\"Transcription:\", transcription['text'])\n",
    "            return transcription['text']\n",
    "        else:\n",
    "            print(\"No speech detected during the recording.\")\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelclass:\n",
    "    def __init__(self):\n",
    "        self.model = ChatOllama(model=\"llama3.2:1b\")\n",
    "        \n",
    "    async def async_stream(self, generator):\n",
    "        for item in generator:\n",
    "            yield item\n",
    "            await asyncio.sleep(0)  # Allows control back to the event loop for async compatibility\n",
    "\n",
    "    async def generate_text(self,text: str):\n",
    "        try:\n",
    "            stream = self.async_stream(self.model.stream(text))\n",
    "            async for line in stream:\n",
    "                print(line.content, end='', flush=True)\n",
    "        except asyncio.CancelledError:\n",
    "            print(\"\\n\\nText generation cancelled.\\n\\n New model call\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTSEngine:\n",
    "    # Different speaker indices from CMU Arctic dataset\n",
    "    VOICE_TYPES = {\n",
    "        'bdl': 0,    # Male voice (BDL) - Deep broadcast voice\n",
    "        'rms': 1,    # Male voice (RMS) - Professional narrative voice\n",
    "        'jmk': 2,    # Male voice (JMK) - Clear articulate voice\n",
    "        'awb': 3,    # Male voice (AWB) - Scottish accent\n",
    "        'ksp': 4,    # Male voice (KSP) - Energetic voice\n",
    "        'rxr': 5,    # Male voice (RXR) - Deeper resonant voice\n",
    "        'aew': 6,    # Male voice (AEW) - Natural conversational voice\n",
    "        'fem': 7     # Male voice (FEM) - Smooth tenor voice\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "        self.model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "        self.vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "        self.embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "        self.current_voice = 'bdl'\n",
    "        self.set_voice(self.current_voice)\n",
    "        \n",
    "    def set_voice(self, voice_type):\n",
    "        if voice_type not in self.VOICE_TYPES:\n",
    "            raise ValueError(f\"Voice type must be one of: {list(self.VOICE_TYPES.keys())}\")\n",
    "        voice_idx = self.VOICE_TYPES[voice_type]\n",
    "        self.speaker_embeddings = torch.tensor(\n",
    "            self.embeddings_dataset[voice_idx][\"xvector\"]\n",
    "        ).unsqueeze(0)\n",
    "        self.current_voice = voice_type\n",
    "        \n",
    "    def speak(self, text, voice_type=None):\n",
    "        if voice_type:\n",
    "            self.set_voice(voice_type)\n",
    "            \n",
    "        inputs = self.processor(text=text, return_tensors=\"pt\")\n",
    "        speech = self.model.generate_speech(\n",
    "            inputs[\"input_ids\"], \n",
    "            self.speaker_embeddings, \n",
    "            vocoder=self.vocoder\n",
    "        )\n",
    "        audio_data = speech.numpy() / np.max(np.abs(speech.numpy()))\n",
    "        sd.play(audio_data, samplerate=17000)\n",
    "        sd.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainClass:\n",
    "    def __init__(self):\n",
    "        self.tts = TTSEngine()\n",
    "        self.processor = AudioProcessor()\n",
    "        self.modelclass = Modelclass()\n",
    "\n",
    "    async def execute(self,querry:str):\n",
    "         task = asyncio.create_task(self.modelclass.generate_text(querry))\n",
    "         try:\n",
    "                await task\n",
    "         except asyncio.CancelledError:\n",
    "                print(\"Task was cancelled successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def main():\n",
    "    task = asyncio.create_task(generate_text(\"tell me a story\"))\n",
    "    try:\n",
    "        await task\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"Task was cancelled successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly await in Jupyter notebook\n",
    "await main()\n",
    "if __name__ == \"__main__\":\n",
    "    processor.record_and_transcribe()\n",
    "if __name__ == \"__main__\":\n",
    "    tts = TTSEngine()\n",
    "    \n",
    "    # Test all voices with the same text\n",
    "    test_text = \"\"\"Once upon a time, in a small village nestled between two great mountains, there lived a young girl named Aria.\"\"\"\n",
    "    tts.speak(test_text, \"rms\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
