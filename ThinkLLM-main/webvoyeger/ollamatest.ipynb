{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a small village nestled in the rolling hills of Tuscany, there was a tiny shop called \"La Bottega dei Sogni\" (The Shop of Dreams). It was owned by an elderly woman named SignCancelling the task...\n",
      "Output generation cancelled.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "# Convert a regular generator to an asynchronous generator\n",
    "async def async_stream(generator):\n",
    "    for item in generator:\n",
    "        yield item\n",
    "        await asyncio.sleep(0)  # Allows control back to the event loop for async compatibility\n",
    "\n",
    "async def generate_text(text: str):\n",
    "    try:\n",
    "        stream = async_stream(model.stream(text))\n",
    "        async for line in stream:\n",
    "            print(line.content, end='', flush=True)\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"Output generation cancelled.\")\n",
    "\n",
    "async def main():\n",
    "    task = asyncio.create_task(generate_text(\"tell me a story\"))\n",
    "    await asyncio.sleep(2)  # Wait for 2 seconds before canceling the task\n",
    "    print(\"Cancelling the task...\")\n",
    "    task.cancel()\n",
    "    try:\n",
    "        await task\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"Task was cancelled successfully.\")\n",
    "\n",
    "# Directly await in Jupyter notebook\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Transcription:  Hello.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "# Load the ASR model\n",
    "asr = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\",device='cpu')\n",
    "\n",
    "# Recording parameters\n",
    "sample_rate = 16000\n",
    "pause_threshold = 0.5  # seconds of silence to consider as a pause\n",
    "energy_threshold = 0.02  # threshold for audio energy\n",
    "max_duration = 3  \n",
    "\n",
    "def record_and_transcribe():\n",
    "    print(\"Listening...\")\n",
    "    audio_buffer = []  # Buffer to hold recorded audio\n",
    "    start_time = time.time()  # Record the start time\n",
    "\n",
    "    while True:\n",
    "        # Record a short chunk of audio\n",
    "        audio_chunk = sd.rec(int(sample_rate * 0.5), samplerate=sample_rate, channels=1, dtype='float32')\n",
    "        sd.wait()  # Wait for the recording to finish\n",
    "\n",
    "        # Calculate the energy of the audio chunk\n",
    "        audio_energy = np.mean(np.abs(audio_chunk))\n",
    "\n",
    "        # Append the audio chunk to the buffer only if it exceeds the energy threshold\n",
    "        if audio_energy > energy_threshold:\n",
    "            audio_buffer.append(audio_chunk.flatten())\n",
    "\n",
    "        # Check if the recording duration has exceeded max_duration\n",
    "        if time.time() - start_time > max_duration:\n",
    "            break\n",
    "\n",
    "        # Check if the last chunk was above the threshold\n",
    "        if len(audio_buffer) > 0:\n",
    "            last_energy = np.mean(np.abs(audio_buffer[-1]))\n",
    "            if last_energy < energy_threshold:\n",
    "                pause_start = time.time()  # Start counting silence duration\n",
    "\n",
    "                # Continue checking for silence for pause_threshold seconds\n",
    "                while True:\n",
    "                    audio_chunk = sd.rec(int(sample_rate * 0.5), samplerate=sample_rate, channels=1, dtype='float32')\n",
    "                    sd.wait()  # Wait for the recording to finish\n",
    "                    \n",
    "                    audio_energy = np.mean(np.abs(audio_chunk))\n",
    "                    \n",
    "                    if audio_energy > energy_threshold:\n",
    "                        print(\"Speaker resumed, continuing recording.\")\n",
    "                        audio_buffer.append(audio_chunk.flatten())\n",
    "                        break\n",
    "                    \n",
    "                    if time.time() - pause_start >= pause_threshold:\n",
    "                        print(\"Pause detected, stopping recording.\")\n",
    "                        break\n",
    "\n",
    "    # Convert the audio buffer to a single numpy array\n",
    "    if audio_buffer:  # Ensure there's audio data to process\n",
    "        audio_data = np.concatenate(audio_buffer)\n",
    "\n",
    "        # Transcribe the recorded audio\n",
    "        transcription = asr(audio_data)\n",
    "        print(\"Transcription:\", transcription['text'])\n",
    "    else:\n",
    "        print(\"No speech detected during the recording.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    record_and_transcribe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Transcription:  Hello, hello, hello.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "from transformers import pipeline\n",
    "import time\n",
    "from collections import deque\n",
    "from threading import Thread, Event\n",
    "import queue\n",
    "\n",
    "class AudioProcessor:\n",
    "    def __init__(self, \n",
    "                 sample_rate=16000,\n",
    "                 chunk_duration=0.1,  # Smaller chunks for faster response\n",
    "                 pause_threshold=0.5,\n",
    "                 energy_threshold=0.02,\n",
    "                 max_duration=3):\n",
    "        \n",
    "        self.sample_rate = sample_rate\n",
    "        self.chunk_duration = chunk_duration\n",
    "        self.chunk_size = int(sample_rate * chunk_duration)\n",
    "        self.pause_threshold = pause_threshold\n",
    "        self.energy_threshold = energy_threshold\n",
    "        self.max_duration = max_duration\n",
    "        \n",
    "        # Use queue for thread-safe audio processing\n",
    "        self.audio_queue = queue.Queue()\n",
    "        self.stop_recording = Event()\n",
    "        \n",
    "        # Initialize ASR model only once\n",
    "        self.asr = pipeline(\"automatic-speech-recognition\", \n",
    "                          model=\"openai/whisper-small\",\n",
    "                          device='cpu')\n",
    "        \n",
    "        # Use deque with maxlen for automatic memory management\n",
    "        max_chunks = int(max_duration / chunk_duration)\n",
    "        self.audio_buffer = deque(maxlen=max_chunks)\n",
    "\n",
    "    def _calculate_energy(self, audio_chunk):\n",
    "        # Vectorized energy calculation\n",
    "        return np.mean(np.abs(audio_chunk))\n",
    "\n",
    "    def _record_audio(self):\n",
    "        \"\"\"Record audio in a separate thread\"\"\"\n",
    "        with sd.InputStream(samplerate=self.sample_rate,\n",
    "                          channels=1,\n",
    "                          dtype=np.float32,\n",
    "                          blocksize=self.chunk_size,\n",
    "                          callback=self._audio_callback):\n",
    "            self.stop_recording.wait()\n",
    "\n",
    "    def _audio_callback(self, indata, frames, time_info, status):\n",
    "        \"\"\"Callback for audio stream processing\"\"\"\n",
    "        if status:\n",
    "            print(f'Error: {status}')\n",
    "        self.audio_queue.put(indata.copy())\n",
    "\n",
    "    def _process_audio_chunk(self, audio_chunk):\n",
    "        \"\"\"Process a single chunk of audio data\"\"\"\n",
    "        energy = self._calculate_energy(audio_chunk)\n",
    "        if energy > self.energy_threshold:\n",
    "            self.audio_buffer.append(audio_chunk.flatten())\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def record_and_transcribe(self):\n",
    "        \"\"\"Main method to record and transcribe audio\"\"\"\n",
    "        print(\"Listening...\")\n",
    "        \n",
    "        # Start recording thread\n",
    "        recording_thread = Thread(target=self._record_audio)\n",
    "        recording_thread.start()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        silence_start = None\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                # Check max duration\n",
    "                if time.time() - start_time > self.max_duration:\n",
    "                    break\n",
    "\n",
    "                # Get audio chunk from queue with timeout\n",
    "                try:\n",
    "                    audio_chunk = self.audio_queue.get(timeout=0.1)\n",
    "                except queue.Empty:\n",
    "                    continue\n",
    "\n",
    "                # Process the chunk\n",
    "                has_speech = self._process_audio_chunk(audio_chunk)\n",
    "                \n",
    "                # Pause detection logic\n",
    "                if not has_speech:\n",
    "                    if silence_start is None:\n",
    "                        silence_start = time.time()\n",
    "                    elif time.time() - silence_start >= self.pause_threshold:\n",
    "                        break\n",
    "                else:\n",
    "                    silence_start = None\n",
    "\n",
    "        finally:\n",
    "            # Clean up\n",
    "            self.stop_recording.set()\n",
    "            recording_thread.join()\n",
    "\n",
    "        # Process recorded audio\n",
    "        if len(self.audio_buffer) > 0:\n",
    "            # Efficient concatenation of all audio chunks\n",
    "            audio_data = np.concatenate(self.audio_buffer)\n",
    "            \n",
    "            # Transcribe\n",
    "            transcription = self.asr(audio_data)\n",
    "            print(\"Transcription:\", transcription['text'])\n",
    "            return transcription['text']\n",
    "        else:\n",
    "            print(\"No speech detected during the recording.\")\n",
    "            return \"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processor = AudioProcessor()\n",
    "    processor.record_and_transcribe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "\n",
    "class TTSEngine:\n",
    "    def __init__(self):\n",
    "        self.processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "        self.model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "        self.vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "        self.speaker_embeddings = torch.tensor(\n",
    "            load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")[7][\"xvector\"]\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "    def speak(self, text):\n",
    "        inputs = self.processor(text=text, return_tensors=\"pt\")\n",
    "        speech = self.model.generate_speech(\n",
    "            inputs[\"input_ids\"], \n",
    "            self.speaker_embeddings, \n",
    "            vocoder=self.vocoder\n",
    "        )\n",
    "        audio_data = speech.numpy() / np.max(np.abs(speech.numpy()))\n",
    "        sd.play(audio_data, samplerate=16500)\n",
    "        sd.wait()\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    tts = TTSEngine()\n",
    "    tts.speak(\"Once upon a time, in a small village nestled between two great mountains, there lived a young girl named Aria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "\n",
    "class TTSEngine:\n",
    "    # Different speaker indices from CMU Arctic dataset\n",
    "    VOICE_TYPES = {\n",
    "        'bdl': 0,    # Male voice (BDL) - Deep broadcast voice\n",
    "        'rms': 1,    # Male voice (RMS) - Professional narrative voice\n",
    "        'jmk': 2,    # Male voice (JMK) - Clear articulate voice\n",
    "        'awb': 3,    # Male voice (AWB) - Scottish accent\n",
    "        'ksp': 4,    # Male voice (KSP) - Energetic voice\n",
    "        'rxr': 5,    # Male voice (RXR) - Deeper resonant voice\n",
    "        'aew': 6,    # Male voice (AEW) - Natural conversational voice\n",
    "        'fem': 7     # Male voice (FEM) - Smooth tenor voice\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "        self.model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "        self.vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "        self.embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "        self.current_voice = 'bdl'\n",
    "        self.set_voice(self.current_voice)\n",
    "        \n",
    "    def set_voice(self, voice_type):\n",
    "        if voice_type not in self.VOICE_TYPES:\n",
    "            raise ValueError(f\"Voice type must be one of: {list(self.VOICE_TYPES.keys())}\")\n",
    "        voice_idx = self.VOICE_TYPES[voice_type]\n",
    "        self.speaker_embeddings = torch.tensor(\n",
    "            self.embeddings_dataset[voice_idx][\"xvector\"]\n",
    "        ).unsqueeze(0)\n",
    "        self.current_voice = voice_type\n",
    "        \n",
    "    def speak(self, text, voice_type=None):\n",
    "        if voice_type:\n",
    "            self.set_voice(voice_type)\n",
    "            \n",
    "        inputs = self.processor(text=text, return_tensors=\"pt\")\n",
    "        speech = self.model.generate_speech(\n",
    "            inputs[\"input_ids\"], \n",
    "            self.speaker_embeddings, \n",
    "            vocoder=self.vocoder\n",
    "        )\n",
    "        audio_data = speech.numpy() / np.max(np.abs(speech.numpy()))\n",
    "        sd.play(audio_data, samplerate=17000)\n",
    "        sd.wait()\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    tts = TTSEngine()\n",
    "    \n",
    "    # Test all voices with the same text\n",
    "    test_text = \"\"\"Once upon a time, in a small village nestled between two great mountains, there lived a young girl named Aria.\"\"\"\n",
    "    tts.speak(test_text, \"rms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts.speak(test_text, \"fem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts.speak(test_text, \"ksp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts.speak(test_text, \"rxr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts.speak(test_text, \"bdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts.speak(test_text, \"rms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
